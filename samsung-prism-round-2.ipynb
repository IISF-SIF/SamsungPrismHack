{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7406242,"sourceType":"datasetVersion","datasetId":4294512},{"sourceId":158055712,"sourceType":"kernelVersion"}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dowloading all the dependencies","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-03-06T16:42:14.662747Z","iopub.execute_input":"2024-03-06T16:42:14.663391Z","iopub.status.idle":"2024-03-06T16:42:14.667662Z","shell.execute_reply.started":"2024-03-06T16:42:14.663353Z","shell.execute_reply":"2024-03-06T16:42:14.666666Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"from ipywidgets import IntProgress\nfrom IPython.display import display\nimport time\n\nmax_count = 10\n\nf = IntProgress(min=0, max=max_count) # instantiate the bar\ndisplay(f) # display the bar\n!pip install -q -U transformers==4.37.2\nf.value+=1\n!pip install -q neural-compressor\nf.value += 1\n!pip install -q flash-attn --no-build-isolation\nf.value += 1\n!pip install -q bitsandbytes==0.41.3 accelerate==0.25.0\nf.value += 1\n!pip install -q langchain\nf.value += 1\n!pip install -q sentence-transformers\nf.value += 1\n!pip install -q indic-nlp-library\nf.value += 1\n!pip install -q -U sacremoses\nf.value += 1\n!pip install -q urduhack\nf.value +=1","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-06T15:45:50.647699Z","iopub.execute_input":"2024-03-06T15:45:50.647995Z","iopub.status.idle":"2024-03-06T15:48:34.997111Z","shell.execute_reply.started":"2024-03-06T15:45:50.647968Z","shell.execute_reply":"2024-03-06T15:48:34.995877Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"IntProgress(value=0, max=10)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ac8bed87c524a5e98eb25a764dc81ad"}},"metadata":{}},{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask 2023.12.0 requires click>=8.1, but you have click 7.1.2 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndistributed 2023.12.0 requires click>=8.0, but you have click 7.1.2 which is incompatible.\nfiona 1.9.5 requires click~=8.0, but you have click 7.1.2 which is incompatible.\nfitter 1.6.0 requires click<9.0.0,>=8.1.6, but you have click 7.1.2 which is incompatible.\nflask 3.0.0 requires click>=8.1.3, but you have click 7.1.2 which is incompatible.\nkfp 2.0.1 requires click<9,>=8.0.0, but you have click 7.1.2 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import git\n\n# Clone the repository from the specified URL to the target directory\nrepo_url = 'https://github.com/VarunGumma/IndicTransTokenizer'\ntarget_directory = '/kaggle/working/sample'\n\n# Clone the repository\ngit.Repo.clone_from(repo_url, target_directory)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:48:34.999853Z","iopub.execute_input":"2024-03-06T15:48:35.000644Z","iopub.status.idle":"2024-03-06T15:48:36.313016Z","shell.execute_reply.started":"2024-03-06T15:48:35.000607Z","shell.execute_reply":"2024-03-06T15:48:36.312089Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<git.repo.base.Repo '/kaggle/working/sample/.git'>"},"metadata":{}}]},{"cell_type":"code","source":"import sys\n# Append the specified directory to the Python sys.path\ntarget_directory = '/kaggle/working/sample'\nsys.path.append(target_directory)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:48:36.314168Z","iopub.execute_input":"2024-03-06T15:48:36.314465Z","iopub.status.idle":"2024-03-06T15:48:36.318979Z","shell.execute_reply.started":"2024-03-06T15:48:36.314440Z","shell.execute_reply":"2024-03-06T15:48:36.318014Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Import required modules and classes\nfrom transformers import BitsAndBytesConfig  # For quantization of the LLM model\nimport torch\nfrom IndicTransTokenizer import IndicProcessor, IndicTransTokenizer # For initialization of the translational model\nfrom transformers import AutoModelForSeq2SeqLM # For initialization of the translational model\nimport transformers  # For the initialization of the LLM Model\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\n\n# Import modules for vector database\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.prompts import PromptTemplate  # Helps in creating a prompt template\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain_community.document_loaders import TextLoader\n\n# Additional imports\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:48:36.321104Z","iopub.execute_input":"2024-03-06T15:48:36.321395Z","iopub.status.idle":"2024-03-06T15:48:41.747319Z","shell.execute_reply.started":"2024-03-06T15:48:36.321372Z","shell.execute_reply":"2024-03-06T15:48:41.746397Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\n# Create a quantization configuration using BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Enable 4-bit quantization during model loading\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:48:41.748538Z","iopub.execute_input":"2024-03-06T15:48:41.749029Z","iopub.status.idle":"2024-03-06T15:48:41.755362Z","shell.execute_reply.started":"2024-03-06T15:48:41.748999Z","shell.execute_reply":"2024-03-06T15:48:41.754414Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Initializing the conversation history\nconversation_history = []","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:48:41.756396Z","iopub.execute_input":"2024-03-06T15:48:41.756692Z","iopub.status.idle":"2024-03-06T15:48:41.765297Z","shell.execute_reply.started":"2024-03-06T15:48:41.756663Z","shell.execute_reply":"2024-03-06T15:48:41.764505Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Initializing OpenAI Whisper to support Audio to Text for more than 100+ languages\n### But it is not that good for Indic languages for which we will be utilizing AI4Bharat Models.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nWhisper_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, use_safetensors=True,device_map=\"cuda:0\",\n    quantization_config=quantization_config\n)\n\nWhisper_processor = AutoProcessor.from_pretrained(model_id)\n\nWhisper_pipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=Whisper_model,\n    tokenizer=Whisper_processor.tokenizer,\n    feature_extractor=Whisper_processor.feature_extractor,\n    max_new_tokens=128,\n    chunk_length_s=30,\n    return_timestamps=True,\n    torch_dtype=torch_dtype,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:48:41.766488Z","iopub.execute_input":"2024-03-06T15:48:41.766766Z","iopub.status.idle":"2024-03-06T15:49:10.945158Z","shell.execute_reply.started":"2024-03-06T15:48:41.766744Z","shell.execute_reply":"2024-03-06T15:49:10.944195Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"124f5f72f5484bbfb96051a44f6fb887"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1ab2a101de54efc8d3b564748056ce2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/3.90k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bcb02e24d3e48a5aa5f6cb89abe041d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc4a2066e691445588d6cc1197c05198"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca1312abb07e45c583bc6d746b0336da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daa3dbd6af7b446da53d7db3f9718893"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cf90a86e89b4336a93faa0490887342"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8011e4700c0246f6917bfde2cadf8920"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10adb54691dc4494945c2c946e047612"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4add4a7a0d945e2bbf9be5002e30fc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26cbad9abefc458c92bcb51e16f9aa82"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initializing Automatic speech recognition using IndicWav2Vec by AI4Bharat\n### As of now we have implemented using only hindi IndicWav2Vec but we can add other indic languages as well.","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\ntranscriber = pipeline(\"automatic-speech-recognition\", model=\"ai4bharat/indicwav2vec-hindi\", device=\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:49:10.946326Z","iopub.execute_input":"2024-03-06T15:49:10.946934Z","iopub.status.idle":"2024-03-06T15:49:45.483930Z","shell.execute_reply.started":"2024-03-06T15:49:10.946907Z","shell.execute_reply":"2024-03-06T15:49:45.482885Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66e87ed429ff4f768070296ed65af9c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6e13d2cde4b4e94bdc5e1a3a944c0f5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/257 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e20282cb9984da4a4aabc232f206c09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/741 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68b7d8f5fee44cfe8fa34eafd8e51481"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b49c81a88b69420295b819b82022b36c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e19171aad6704f30943688683976e5ef"}},"metadata":{}},{"name":"stderr","text":"Could not load the `decoder` for ai4bharat/indicwav2vec-hindi. Defaulting to raw CTC. Error: No module named 'kenlm'\nTry to install `kenlm`: `pip install kenlm\nTry to install `pyctcdecode`: `pip install pyctcdecode\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initialiazation of the AI4Bharat IndicTrans2\n### This model has amazing translational capabilities which will be helpful to us to answer a wide diversity of people","metadata":{}},{"cell_type":"markdown","source":"### Indic to English","metadata":{}},{"cell_type":"code","source":"# Instantiate an IndicTransTokenizer for transliterating from Indic to English\ntokenizer_indic_to_en = IndicTransTokenizer(direction=\"indic-en\")\n\n# Instantiate an IndicProcessor for processing transliteration from Indic to English\nip_indic_to_en = IndicProcessor(inference=True)\n\n# Load a pre-trained model for transliteration from Indic to English using AutoModelForSeq2SeqLM\nmodel_indic_to_en = AutoModelForSeq2SeqLM.from_pretrained(\n    \"ai4bharat/indictrans2-indic-en-1B\",\n    trust_remote_code=True,\n    device_map=\"cuda:0\",\n    quantization_config=quantization_config  # Quantization configuration for the model\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:49:45.485505Z","iopub.execute_input":"2024-03-06T15:49:45.485877Z","iopub.status.idle":"2024-03-06T15:50:09.603035Z","shell.execute_reply.started":"2024-03-06T15:49:45.485840Z","shell.execute_reply":"2024-03-06T15:50:09.602253Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bce4bd7ba4a41c0b3f16f8933252b26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_indictrans.py:   0%|          | 0.00/14.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15052d5ae926472993c5851e03c137b7"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-indic-en-1B:\n- configuration_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_indictrans.py:   0%|          | 0.00/61.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dccf06f0e2ed4050b646b9d93bafaf45"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-indic-en-1B:\n- modeling_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/4.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e7cb3bb2f6242acb89e0467bf768954"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"978512b694d1488e856e72f8c475b665"}},"metadata":{}}]},{"cell_type":"markdown","source":"### English to Indic","metadata":{}},{"cell_type":"code","source":"# Load the tokenizer using IndicTransTokenizer for transliterating from English to Indic\ntokenizer_en_to_indic = IndicTransTokenizer(direction=\"en-indic\")\n\n# Instantiate an IndicProcessor for processing transliteration from English to Indic\nip_en_to_indic = IndicProcessor(inference=True)\n\n# Load a pre-trained model for transliteration from English to Indic using AutoModelForSeq2SeqLM\nmodel_en_to_indic = AutoModelForSeq2SeqLM.from_pretrained(\n    \"ai4bharat/indictrans2-en-indic-dist-200M\",\n    trust_remote_code=True,\n    device_map=\"cuda:0\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:50:09.607113Z","iopub.execute_input":"2024-03-06T15:50:09.608038Z","iopub.status.idle":"2024-03-06T15:50:16.369413Z","shell.execute_reply.started":"2024-03-06T15:50:09.607467Z","shell.execute_reply":"2024-03-06T15:50:16.368394Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cb53de1fc144c628db45430d6bee4da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_indictrans.py:   0%|          | 0.00/14.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3068b364cd64da7ae8eeed726e53595"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M:\n- configuration_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_indictrans.py:   0%|          | 0.00/61.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fc28356fa1444e0bbd81cc8d8ccf421"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M:\n- modeling_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28190540f6584f5a8affc78cab9a9e99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47e7ed932091464dbcdc8c272a1daa91"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Initialization of LLaVa for visual+textual support","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\n\nVQA_pipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config})","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:50:16.371065Z","iopub.execute_input":"2024-03-06T15:50:16.371631Z","iopub.status.idle":"2024-03-06T15:52:25.772243Z","shell.execute_reply.started":"2024-03-06T15:50:16.371584Z","shell.execute_reply":"2024-03-06T15:52:25.771334Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad8d4f8d0384ea7b1ebde9f367fbd53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/70.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1e206c612a145c19849ef361a12cc0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a62896a50aab46079b65255fcb9d43b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"112eacb8d1eb459ab4ae4714b965b309"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3621d3021dc143be9137581253699cee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ea7f3be587742c19dd2227e99b6e017"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba86787189624954a9ac87b1d95a29e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f795bb7ecf224b52bfc7390fbf3f0a09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c9e8e1472f54e14b105f8513882c382"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29828eec6c754aaa8f6815c6afbd9b68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e688f92acb414c93b6bab946317fb2a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"172c3b6079804ebda8563e95433feb78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1fec4e8c476434195e42105586e43ad"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"679b6fd65d5f4b88830c93cd35468608"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Initialization of Mistral-Instruct-v0.2","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\nLLM_pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\", model_kwargs={\"quantization_config\": quantization_config})","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:52:25.773513Z","iopub.execute_input":"2024-03-06T15:52:25.773807Z","iopub.status.idle":"2024-03-06T15:54:33.650093Z","shell.execute_reply.started":"2024-03-06T15:52:25.773781Z","shell.execute_reply":"2024-03-06T15:54:33.649098Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2126f5821468460d9cefeea380510a1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c519534bf234f77ab28b10c2f0d280c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2c3e211e60e4908a407a8fc1769541e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4bc5f1d76a242d38c4d6d1877137556"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fbc781227394a71bcea1007123cb32b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2bd2134913147f890acbab481752b23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c3b95581e264a15ba3dd57c06b1f6c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31d85c9d772f4569af66a2e3a6d71bbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0dd7c93d1ae43569d98025e38b62314"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac4f70ce438e4b73b97d92faf84f6f7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4af83d39f61548f6a6b0e2c892140af4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8477cf7379a747abaef87ed84043b003"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Conversation History Tools","metadata":{}},{"cell_type":"code","source":"def get_history(k=3):\n    \"\"\"\n    Retrieve conversation history for contextual understanding.\n\n    Parameters:\n    - k (int): The number of conversation tiles to include in the context (default is 3).\n\n    Returns:\n    str: A string containing the conversation history up to the specified number of tiles.\n    \"\"\"\n\n    # Initialize an empty string to store the conversation history\n    conversation = \"\"\n\n    # Get the length of the conversation history\n    l_conv = len(conversation_history)\n\n    # If the length of the conversation history is greater than k, retrieve the last k tiles\n    if l_conv > k:\n        for i in range(l_conv - k, l_conv):\n            conversation += conversation_history[i]\n    else:\n        # If the length is less than or equal to k, include the entire conversation history\n        for i in conversation_history:\n            conversation += i\n\n    # Return the concatenated conversation history string\n    return conversation","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:54:33.652038Z","iopub.execute_input":"2024-03-06T15:54:33.652356Z","iopub.status.idle":"2024-03-06T15:54:33.659915Z","shell.execute_reply.started":"2024-03-06T15:54:33.652330Z","shell.execute_reply":"2024-03-06T15:54:33.659000Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Query Processing","metadata":{}},{"cell_type":"markdown","source":"### Taking the query from the user","metadata":{}},{"cell_type":"markdown","source":"### Convert the query from Indic to English","metadata":{}},{"cell_type":"code","source":"def translate_indic_to_english(user_input,source_language,target_language):\n    # Record the start time for performance measurement\n    begin = time.time()\n    # Check if the source language is different from the target language for transliteration\n    if not (source_language == target_language):\n        # Preprocess the user input batch for transliteration\n        batch = ip_indic_to_en.preprocess_batch([user_input], src_lang=source_language, tgt_lang=target_language)\n        batch = tokenizer_indic_to_en(batch, src=True, return_tensors=\"pt\")\n\n        # Generate transliterations using the pre-trained model\n        with torch.inference_mode():\n            user_input_translated = model_indic_to_en.generate(\n                batch.input_ids.to(\"cuda\"), num_beams=5, num_return_sequences=1, max_length=256\n            )\n\n        # Decode the generated sequences and post-process the transliterations\n        user_input_translated = tokenizer_indic_to_en.batch_decode(user_input_translated, src=False)\n        user_input_translated = ip_indic_to_en.postprocess_batch(user_input_translated, lang=target_language)\n\n        # Print the transliterated user input\n        # print(user_input_translated)\n    else:\n        # If the source and target languages are the same, no transliteration is needed\n        user_input_translated = [user_input]\n    end = time.time()\n    return user_input_translated,end-begin","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:54:33.661606Z","iopub.execute_input":"2024-03-06T15:54:33.662004Z","iopub.status.idle":"2024-03-06T15:54:33.696197Z","shell.execute_reply.started":"2024-03-06T15:54:33.661968Z","shell.execute_reply":"2024-03-06T15:54:33.695453Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Converting English output from the Model back to Hindi","metadata":{}},{"cell_type":"code","source":"def translate_english_to_indic(en_text,source_language,target_language):\n    begin = time.time()\n    # Translate English text to Hindi if the source and target languages are different\n    if not (source_language == target_language):\n        # Preprocess the English text for translation\n        batch = ip_en_to_indic.preprocess_batch([en_text], src_lang=target_language, tgt_lang=source_language)\n        batch = tokenizer_en_to_indic(batch, src=True, return_tensors=\"pt\")\n\n        # Generate translations using the pre-trained model\n        with torch.inference_mode():\n            outputs = model_en_to_indic.generate(\n                batch.input_ids.to(\"cuda\"), num_beams=2, num_return_sequences=1, max_length=256\n            )\n\n        # Decode the generated sequences and post-process the translations\n        outputs = tokenizer_en_to_indic.batch_decode(outputs, src=False)\n        outputs = ip_en_to_indic.postprocess_batch(outputs, lang=source_language)\n    else:\n        # If the source and target languages are the same, no translation is needed\n        outputs = [en_text]\n\n    # Record the end time for performance measurement\n    end = time.time()\n\n    # Measure and print the elapsed time for the operation\n    elapsed_time = end - begin\n    \n    return outputs,elapsed_time","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:54:33.697480Z","iopub.execute_input":"2024-03-06T15:54:33.697732Z","iopub.status.idle":"2024-03-06T15:54:33.710856Z","shell.execute_reply.started":"2024-03-06T15:54:33.697703Z","shell.execute_reply":"2024-03-06T15:54:33.710044Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"conversation_history = []","metadata":{"execution":{"iopub.status.busy":"2024-03-06T15:54:33.712043Z","iopub.execute_input":"2024-03-06T15:54:33.712565Z","iopub.status.idle":"2024-03-06T15:54:33.723540Z","shell.execute_reply.started":"2024-03-06T15:54:33.712532Z","shell.execute_reply":"2024-03-06T15:54:33.722647Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import re\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-03-06T16:06:14.958388Z","iopub.execute_input":"2024-03-06T16:06:14.958768Z","iopub.status.idle":"2024-03-06T16:06:14.963147Z","shell.execute_reply.started":"2024-03-06T16:06:14.958738Z","shell.execute_reply":"2024-03-06T16:06:14.962207Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def conversation(file_path_audio, language_choice, is_audio=False, file_path_image=None, time_it=True):\n    \"\"\"\n    Conducts a conversation with the chatbot, translating user input, retrieving context, and generating responses.\n\n    Parameters:\n    - user_input (str): The input provided by the user in an Indic language.\n    - time_it (bool): A flag indicating whether to print the time taken for each step (default is True).\n\n    Returns:\n    None\n    \"\"\"\n    # Define a dictionary mapping languages to their corresponding Indic2Trans language codes\n    languages = {\n        \"Assamese\":\"asm_Beng\",\n        \"Kashmiri (Arabic)\":\"kas_Arab\",\n        \"Punjabi\":\"pan_Guru\",\n        \"Bengali\":\"ben_Beng\",\n        \"Kashmiri (Devanagari)\":\"kas_Deva\",\n        \"Sanskrit\":\"san_Deva\",\n        \"Bodo\":\"brx_Deva\",\n        \"Maithili\":\"mai_Deva\",\n        \"Santali\":\"sat_Olck\",\n        \"Dogri\":\"doi_Deva\",\n        \"Malayalam\":\"mal_Mlym\",\n        \"Sindhi (Arabic)\":\"snd_Arab\",\n        \"Marathi\":\"mar_Deva\",\n        \"English\":\"eng_Latn\",\n        \"Sindhi (Devanagari)\":\"snd_Deva\",\n        \"Konkani\":\"gom_Deva\",\n        \"Manipuri (Bengali)\":\"mni_Beng\",\n        \"Tamil\":\"tam_Taml\",\n        \"Gujarati\":\"guj_Gujr\",\n        \"Manipuri (Meitei)\":\"mni_Mtei\",\n        \"Telugu\":\"tel_Telu\",\n        \"Hindi\":\"hin_Deva\",\n        \"Nepali\":\"npi_Deva\",\n        \"Urdu\":\"urd_Arab\",\n        \"Kannada\":\"kan_Knda\",\n        \"Odia\":\"ory_Orya\"\n    }\n    # Select the source language for transliteration based on the user's choice\n    source_language = languages[language_choice]\n\n    # Set the target language for transliteration to English\n    target_language = \"eng_Latn\"\n    # ASR model to convert the audio to text\n    if is_audio:\n        if language_choice==\"English\":\n            # Will work for all latin languages\n            user_input = Whisper_pipe(file_path_audio)\n            user_input = user_input[\"text\"]\n        else:\n            # Specially introduced for Indic Languages\n            user_input = transcriber(file_path_audio)\n            user_input = user_input[\"text\"]\n    else:\n        user_input = file_path_audio\n\n    # Translate user input from Indic to English\n    user_input_translated, t1_time = translate_indic_to_english(user_input,source_language,target_language)\n\n    # Record the start time for performance measurement\n    begin = time.time()\n    \n    if file_path_image!=None:\n        image = Image.open(file_path_image)\n        prompt = f\"\"\"USER: <image>\\nYou are a helpful visual and language assistant who answers to users queries in one to two sentences. {user_input_translated[0]} \\nASSISTANT: \"\"\"\n        conversation_history.append({\"role\": \"user\", \"content\": user_input_translated})\n        out_pipe = VQA_pipe(image,prompt=prompt, generate_kwargs={\"max_new_tokens\": 64,\"temperature\":0.75,\"do_sample\":True})\n        generated_text = out_pipe[0]['generated_text']\n        model_reply = \"\"\n        flag = False\n        for i in generated_text.split(\" \"):\n            if flag:\n                model_reply += i + \" \"\n            if i == \"\\nASSISTANT:\":\n                flag = True\n    else:\n        prompt = f\"\"\"<s>[INST] You are a helpful chatbot which answers to users queries in one to two sentences.\\n### User Query: {user_input_translated[0]} [/INST]\"\"\"\n        conversation_history.append({\"role\": \"user\", \"content\": user_input_translated})\n        out_pipe = LLM_pipe(prompt, max_new_tokens=128)\n        generated_text = out_pipe[0]['generated_text']\n        model_reply = \"\"\n        flag = False\n        for i in generated_text.split(\" \"):\n            if flag:\n                model_reply += i + \" \"\n            if i == \"[/INST]\":\n                flag = True\n    \n    # Calculate the time taken for the LLM generation\n    llm_time = time.time() - begin\n\n    # Update conversation history with user input and assistant response\n    conversation_history.append({\"role\": \"assistant\", \"content\": model_reply})\n\n    # Translate the generated English text back to Indic\n    output, t2_time = translate_english_to_indic(model_reply,source_language,target_language)\n\n    # Print the translated output\n    return output[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-06T16:47:20.888612Z","iopub.execute_input":"2024-03-06T16:47:20.889082Z","iopub.status.idle":"2024-03-06T16:47:20.907653Z","shell.execute_reply.started":"2024-03-06T16:47:20.889044Z","shell.execute_reply":"2024-03-06T16:47:20.906630Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"# Markdown Script","metadata":{}},{"cell_type":"code","source":"import pathlib\nimport textwrap\n\n# Import the generative AI module from the google package\nimport google.generativeai as genai\n\n# Import necessary display modules from IPython\nfrom IPython.display import display, Markdown\n\ndef to_markdown(text):\n    \"\"\"\n    Convert plain text to Markdown format.\n\n    This function takes a plain text input and converts it to Markdown format.\n    It also replaces bullet points with proper Markdown list syntax.\n\n    Args:\n        text (str): The plain text to be converted to Markdown.\n\n    Returns:\n        Markdown: The converted Markdown text.\n\n    Example:\n        >>> plain_text = \"This is a bullet point:\\n• Item 1\\n• Item 2\"\n        >>> markdown_output = to_markdown(plain_text)\n        >>> print(markdown_output)\n        > This is a bullet point:\n        >   * Item 1\n        >   * Item 2\n    \"\"\"\n    # Replace special character '•' with proper Markdown list syntax\n    text = text.replace('•', '  *')\n    \n    # Indent the text with '>' to format it as a blockquote\n    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))","metadata":{"execution":{"iopub.status.busy":"2024-03-06T16:39:42.650400Z","iopub.execute_input":"2024-03-06T16:39:42.650826Z","iopub.status.idle":"2024-03-06T16:39:43.012186Z","shell.execute_reply.started":"2024-03-06T16:39:42.650796Z","shell.execute_reply":"2024-03-06T16:39:43.011437Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"# Running Samples","metadata":{}},{"cell_type":"code","source":"%%time\nuser_input1 = \"/kaggle/input/audiotest/Recording.mp3\"\nto_markdown(conversation(user_input1, is_audio=True, language_choice=\"Hindi\"))","metadata":{"execution":{"iopub.status.busy":"2024-03-06T16:47:25.595008Z","iopub.execute_input":"2024-03-06T16:47:25.595433Z","iopub.status.idle":"2024-03-06T16:47:31.081256Z","shell.execute_reply.started":"2024-03-06T16:47:25.595404Z","shell.execute_reply":"2024-03-06T16:47:31.080371Z"},"trusted":true},"execution_count":115,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 5.3 s, sys: 88.8 ms, total: 5.39 s\nWall time: 5.48 s\n","output_type":"stream"},{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"> अपने स्थान के आधार पर निकटतम आधार नामांकन केंद्र खोजने के लिए यू. आई. डी. ए. आई. की वेबसाइट पर आधार नामांकन और अद्यतन लोकेटर का उपयोग करें।"},"metadata":{}}]},{"cell_type":"code","source":"%%time\nuser_input2 = \"Can you give me 2-3 cool instagram captions for this image?\"\nto_markdown(conversation(file_path_audio=user_input2, file_path_image=\"/kaggle/input/imagestotestllava/InstagramGirl2.jpg\",language_choice=\"English\"))","metadata":{"execution":{"iopub.status.busy":"2024-03-06T16:47:35.198464Z","iopub.execute_input":"2024-03-06T16:47:35.199340Z","iopub.status.idle":"2024-03-06T16:47:39.344108Z","shell.execute_reply.started":"2024-03-06T16:47:35.199305Z","shell.execute_reply":"2024-03-06T16:47:39.343171Z"},"trusted":true},"execution_count":116,"outputs":[{"name":"stdout","text":"CPU times: user 3.8 s, sys: 355 ms, total: 4.15 s\nWall time: 4.14 s\n","output_type":"stream"},{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"> 1. \"Purple vibes.\"\n> 2. \"Purple world, my friend.\"\n> 3. \"Let's go purple.\" "},"metadata":{}}]},{"cell_type":"code","source":"%%time\nuser_input3 = \"இது எந்த இடத்தின் படம்?\"\nto_markdown(conversation(file_path_audio=user_input3, file_path_image=\"/kaggle/input/imagestotestllava/TimesSquareNewYork.jpg\",language_choice=\"Tamil\"))","metadata":{"execution":{"iopub.status.busy":"2024-03-06T16:51:28.639392Z","iopub.execute_input":"2024-03-06T16:51:28.640112Z","iopub.status.idle":"2024-03-06T16:51:31.930151Z","shell.execute_reply.started":"2024-03-06T16:51:28.640078Z","shell.execute_reply":"2024-03-06T16:51:31.929266Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"CPU times: user 2.9 s, sys: 396 ms, total: 3.3 s\nWall time: 3.28 s\n","output_type":"stream"},{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"> அந்த இடம் நியூயார்க் நகரம்."},"metadata":{}}]},{"cell_type":"code","source":"%%time\nuser_input4 = \"¿Qué se muestra en esta imagen?\"\nto_markdown(conversation(file_path_audio=user_input4, file_path_image=\"/kaggle/input/imagestotestllava/SolarSystem.jpg\", language_choice=\"English\"))","metadata":{"execution":{"iopub.status.busy":"2024-03-06T16:49:14.658960Z","iopub.execute_input":"2024-03-06T16:49:14.659424Z","iopub.status.idle":"2024-03-06T16:49:19.578461Z","shell.execute_reply.started":"2024-03-06T16:49:14.659389Z","shell.execute_reply":"2024-03-06T16:49:19.577571Z"},"trusted":true},"execution_count":120,"outputs":[{"name":"stdout","text":"CPU times: user 4.55 s, sys: 377 ms, total: 4.93 s\nWall time: 4.91 s\n","output_type":"stream"},{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"> \n> En esta imagen se muestra la posición de varios planetas en el universo, junto con la posición de la Tierra. Además, se pueden observar algunos planetas y estrellas. "},"metadata":{}}]},{"cell_type":"code","source":"%%time\nuser_input5 = \"Can you give me the recipe for mayoniesse\"\nto_markdown(conversation(file_path_audio=user_input5,language_choice=\"English\"))","metadata":{"execution":{"iopub.status.busy":"2024-03-06T16:49:19.580256Z","iopub.execute_input":"2024-03-06T16:49:19.580650Z","iopub.status.idle":"2024-03-06T16:49:26.289773Z","shell.execute_reply.started":"2024-03-06T16:49:19.580617Z","shell.execute_reply":"2024-03-06T16:49:26.288819Z"},"trusted":true},"execution_count":121,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 6.62 s, sys: 85 ms, total: 6.71 s\nWall time: 6.7 s\n","output_type":"stream"},{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"> I'm glad you asked for a mayonnaise recipe! Here it is: In a blender or food processor, combine 1 cup of vegetable oil, 1 egg, 1 tablespoon of mustard, 1 tablespoon of white vinegar or lemon juice, and a pinch of salt. Blend until thick and creamy. Taste and add more salt or acidity if needed. Refrigerate before serving. "},"metadata":{}}]}]}